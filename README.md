# Deep Learning Coursework

This repository contains the practical work and mini project completed during the Deep Learning course. Each session explores different deep learning techniques, from implementing models from scratch to deploying them using modern frameworks.

## Table of Contents

- [TP-01: Training MLP from Scratch](#tp-01-training-mlp-from-scratch)
- [TP-02: MLP with Keras and Hyperparameter Optimization](#tp-02-mlp-with-keras-and-hyperparameter-optimization)
- [TP-03: CNN from Scratch](#tp-03-cnn-from-scratch)
- [TP-04: Transfer Learning](#tp-04-transfer-learning)
- [Mini Project: Semantic Segmentation & Streamlit Deployment](#mini-project-semantic-segmentation--streamlit-deployment)
- [How to View and Edit on Kaggle](#how-to-view-and-edit-on-kaggle)

---

## TP-01: Training MLP from Scratch
In this practical session, we build a **Multilayer Perceptron (MLP)** from scratch using Python and NumPy. The goal is to understand the fundamental building blocks of an MLP, including forward and backward propagation.

## TP-02: MLP with Keras and Hyperparameter Optimization
This session focuses on implementing an MLP using the **Keras** framework. We also perform **hyperparameter optimization** to fine-tune model parameters like learning rate, number of neurons, and batch size for improved performance.

## TP-03: CNN from Scratch
In this TP, we develop a **Convolutional Neural Network (CNN)** from scratch. We implement the convolutional layers, pooling layers, and fully connected layers manually using Python, and compare the modelâ€™s performance with Keras-based implementations.

## TP-04: Transfer Learning
We implement **Transfer Learning** by using pre-trained models (such as VGG16 and ResNet) and fine-tuning them on a custom dataset. Transfer learning significantly improves model performance and reduces training time by leveraging pre-trained knowledge.

## Mini Project: Semantic Segmentation & Streamlit Deployment
The mini project involves performing **Semantic Segmentation** using modern deep learning architectures. After training the model, we deploy it using **Streamlit**, creating a simple web application that can be used for real-time image segmentation.

---

## How to View and Edit on Kaggle

You can also view and edit the notebooks directly on **Kaggle**. To do this:

1. Upload the notebook or copy the repository to your Kaggle environment.
2. Open the notebook in **Kaggle** and make your edits.
3. Run the notebook directly on Kaggle's infrastructure without setting up any environment locally.

Kaggle provides an easy way to experiment with deep learning models and access GPU resources for faster training.

---

Feel free to explore the repository and make modifications as needed!
